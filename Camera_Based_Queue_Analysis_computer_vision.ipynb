{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-18T19:58:15.835221Z",
          "iopub.status.busy": "2025-05-18T19:58:15.834973Z",
          "iopub.status.idle": "2025-05-18T19:59:45.091889Z",
          "shell.execute_reply": "2025-05-18T19:59:45.090963Z",
          "shell.execute_reply.started": "2025-05-18T19:58:15.835198Z"
        },
        "trusted": true,
        "id": "yuE4GXFErzCi"
      },
      "outputs": [],
      "source": [
        "pip install ultralytics opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-18T19:59:45.094520Z",
          "iopub.status.busy": "2025-05-18T19:59:45.094223Z",
          "iopub.status.idle": "2025-05-18T19:59:51.357966Z",
          "shell.execute_reply": "2025-05-18T19:59:51.357387Z",
          "shell.execute_reply.started": "2025-05-18T19:59:45.094498Z"
        },
        "trusted": true,
        "id": "8HBgdLgbrzCk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "import imageio\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRctzq0-rzCl"
      },
      "source": [
        "VIDEO CREDIT.https://youtu.be/5rkwqp6nnr4?si=jdkWhgZ9adCqKLTk by MARCH NETWORKS:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUPmfuMXrzCn"
      },
      "source": [
        "Although I had only one video, I simulated a multi-camera setup by dividing the video into two zones — representing different camera views. By tracking people as they move from one zone to another using unique IDs, I ensured accurate counting without duplication and enabled real-time queue monitoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-18T19:59:51.358874Z",
          "iopub.status.busy": "2025-05-18T19:59:51.358608Z",
          "iopub.status.idle": "2025-05-18T20:00:41.946677Z",
          "shell.execute_reply": "2025-05-18T20:00:41.945858Z",
          "shell.execute_reply.started": "2025-05-18T19:59:51.358859Z"
        },
        "trusted": true,
        "id": "lbNM_znBrzCo"
      },
      "outputs": [],
      "source": [
        "input_path = r\"/kaggle/input/quevideos/que.mp4\"\n",
        "output_path = r\"/kaggle/working/step4_fully_customizable_output.mp4\"\n",
        "\n",
        "def get_video_details(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Cannot open video file.\")\n",
        "        return\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    duration = frame_count / fps if fps > 0 else 0\n",
        "\n",
        "    # print details\n",
        "    print(f\"Resolution: {width}x{height}\")\n",
        "    print(f\"FPS: {fps}\")\n",
        "    print(f\"Total Frames: {frame_count}\")\n",
        "    print(f\"Duration (seconds): {duration:.2f}\")\n",
        "\n",
        "    if height >= 1080:\n",
        "        quality = \"1080p (Full HD or higher)\"\n",
        "    elif height >= 720:\n",
        "        quality = \"720p (HD)\"\n",
        "    elif height >= 480:\n",
        "        quality = \"480p (SD)\"\n",
        "    else:\n",
        "        quality = \"Lower than 480p\"\n",
        "\n",
        "    print(f\"Quality Category: {quality}\")\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "\n",
        "get_video_details(input_path)\n",
        "\n",
        "def display_middle_300(video_path):\n",
        "    reader = imageio.get_reader(video_path)\n",
        "    total_frames = reader.count_frames()\n",
        "    start_frame = max(0, total_frames // 2 - 150)\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    plt.axis('off')\n",
        "\n",
        "    mov = []\n",
        "    for i, frame in enumerate(reader):\n",
        "        if i < start_frame:\n",
        "            continue\n",
        "        if i >= start_frame + 300:\n",
        "            break\n",
        "        img = plt.imshow(frame, animated=True)\n",
        "        plt.axis('off')\n",
        "        mov.append([img])\n",
        "\n",
        "    reader.close()\n",
        "\n",
        "    anime = animation.ArtistAnimation(fig, mov, interval=50, repeat_delay=1000)\n",
        "    plt.close()\n",
        "    return anime\n",
        "\n",
        "HTML(display_middle_300(input_path).to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2adqCNZ7rzCo"
      },
      "source": [
        "## Conceptualize the Camera Setup\n",
        "\n",
        "### Video Source\n",
        "\n",
        "The video used in this project is taken from **YouTube**, uploaded by **March Networks**. It shows a queue from a **top-side camera view** set up above a cashier window.\n",
        "\n",
        "\n",
        "### Camera Setup (based on the video)\n",
        "\n",
        "* The camera is placed **above and behind the cashier**.\n",
        "* It looks down at the line of people **coming from the left and moving toward the right**.\n",
        "* We can see people’s **full bodies** from this angle.\n",
        "\n",
        "### Number of Cameras\n",
        "\n",
        "* The original task asks for at least **2 camera views**, but I only had **one video**.\n",
        "* To solve this, I **split the video frame into two zones**:\n",
        "\n",
        "  * **Camera 1** (left side) = where people enter the queue.\n",
        "  * **Camera 2** (right side) = where people reach the counter.\n",
        "* These zones act like **two different cameras**, helping us **track people across the queue**.\n",
        "\n",
        "\n",
        "### Visibility and Method Used\n",
        "\n",
        "* I used **YOLOv8m** (an AI model) to **detect and track people** by drawing boxes around them.\n",
        "* It follows each person using an ID so no one gets counted twice.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-18T20:08:50.255999Z",
          "iopub.status.busy": "2025-05-18T20:08:50.255272Z",
          "iopub.status.idle": "2025-05-18T20:11:18.979162Z",
          "shell.execute_reply": "2025-05-18T20:11:18.978141Z",
          "shell.execute_reply.started": "2025-05-18T20:08:50.255971Z"
        },
        "scrolled": true,
        "trusted": true,
        "id": "bkH368fvrzCp"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "# Load YOLOv8 model\n",
        "model = YOLO(\"yolov8m.pt\")\n",
        "output_path = r\"/kaggle/working/step4_fully_customizable_output.mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(input_path)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "zone_divider = width // 2\n",
        "id_first_seen = {}\n",
        "frame_number = 0\n",
        "\n",
        "# configuration controls\n",
        "BOX_THICKNESS = 2\n",
        "BOX_FONT_SCALE = 0.5\n",
        "BOX_FONT_THICKNESS = 1\n",
        "BOX_TEXT_BG_PADDING = 3\n",
        "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "CASHIER_BOX_THICKNESS = 2\n",
        "CASHIER_BOX_COLOR = (0, 0, 255)  # Red\n",
        "CASHIER_TEXT = \"Cashier\"\n",
        "\n",
        "COUNT_FONT_SCALE = 0.8\n",
        "COUNT_FONT_THICKNESS = 2\n",
        "ZONE1_COLOR = (255, 100, 100)\n",
        "ZONE2_COLOR = (100, 255, 100)\n",
        "\n",
        "\n",
        "QUEUE_AREA_COLOR = (0, 255, 255)\n",
        "QUEUE_AREA_THICKNESS = 2\n",
        "QUEUE_COLOR = (255, 255, 0)\n",
        "\n",
        "\n",
        "# cashier area (bottom right corner)\n",
        "cashier_area = (int(width * 0.75), int(height * 0.7), width, height)\n",
        "\n",
        "# define the queue area as the full width and vertical region just in front of cashier area\n",
        "queue_area_top = cashier_area[1] - 135 # if cashier_area[1] - 150 > 0 else 0\n",
        "queue_area_bottom = cashier_area[3]\n",
        "queue_area = (0, queue_area_top, width, queue_area_bottom)\n",
        "\n",
        "def generate_color(id_num):\n",
        "    random.seed(id_num)\n",
        "    return (random.randint(50, 255), random.randint(50, 255), random.randint(50, 255))\n",
        "\n",
        "def is_inside_area(cx, cy, area):\n",
        "    x1, y1, x2, y2 = area\n",
        "    return x1 <= cx <= x2 and y1 <= cy <= y2\n",
        "\n",
        "def boxes_intersect(box1, box2):\n",
        "    x1_min, y1_min, x1_max, y1_max = box1\n",
        "    x2_min, y2_min, x2_max, y2_max = box2\n",
        "\n",
        "    if x1_max < x2_min or x2_max < x1_min:\n",
        "        return False\n",
        "    if y1_max < y2_min or y2_max < y1_min:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# start processing the video\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_number += 1\n",
        "\n",
        "    results = model.track(frame, persist=True, classes=[0], verbose=False)\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    zone1_count = 0\n",
        "    zone2_count = 0\n",
        "    total_in_queue = 0\n",
        "\n",
        "    for box in results[0].boxes:\n",
        "        if box.id is None:\n",
        "            continue\n",
        "\n",
        "        track_id = int(box.id.item())\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
        "\n",
        "        in_cashier = is_inside_area(cx, cy, cashier_area)\n",
        "        in_queue_area = boxes_intersect((x1, y1, x2, y2), queue_area)\n",
        "\n",
        "        # start timer for new IDs\n",
        "        if track_id not in id_first_seen:\n",
        "            id_first_seen[track_id] = frame_number\n",
        "\n",
        "        time_waited = (frame_number - id_first_seen[track_id]) / fps  # in seconds\n",
        "        confidence = box.conf.item() if box.conf is not None else 0\n",
        "\n",
        "        # only draw boxes if person is in cashier or queue area\n",
        "        if in_cashier or in_queue_area:\n",
        "\n",
        "            box_color = CASHIER_BOX_COLOR if in_cashier else generate_color(track_id)\n",
        "            label_text = f\"ID {track_id} | {confidence * 100:.1f}% | \"\n",
        "\n",
        "            if in_cashier:\n",
        "                label_text += \"Cashier\"\n",
        "            else:\n",
        "                label_text += f\"Queue | {time_waited:.1f}s\"\n",
        "\n",
        "                # Count zones for queue\n",
        "                if cx < zone_divider:\n",
        "                    zone1_count += 1\n",
        "                else:\n",
        "                    zone2_count += 1\n",
        "                total_in_queue += 1\n",
        "\n",
        "            # draw bounding box\n",
        "            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), box_color, BOX_THICKNESS)\n",
        "\n",
        "            # draw label background and text\n",
        "            (text_width, text_height), _ = cv2.getTextSize(label_text, FONT, BOX_FONT_SCALE, BOX_FONT_THICKNESS)\n",
        "            text_x = x1\n",
        "            text_y = y1 + text_height + 5 if y1 + text_height + 5 < y2 else y1 - 10\n",
        "            bg_top_left = (text_x, text_y - text_height - BOX_TEXT_BG_PADDING)\n",
        "            bg_bottom_right = (text_x + text_width, text_y + BOX_TEXT_BG_PADDING)\n",
        "            cv2.rectangle(annotated_frame, bg_top_left, bg_bottom_right, box_color, -1)\n",
        "            cv2.putText(annotated_frame, label_text, (text_x, text_y), FONT, BOX_FONT_SCALE, (255, 255, 255), BOX_FONT_THICKNESS)\n",
        "\n",
        "\n",
        "    # draw cashier area\n",
        "    cv2.rectangle(annotated_frame,\n",
        "                  (cashier_area[0], cashier_area[1]),\n",
        "                  (cashier_area[2], cashier_area[3]),\n",
        "                  CASHIER_BOX_COLOR, CASHIER_BOX_THICKNESS)\n",
        "    cv2.putText(annotated_frame, \"Cashier Area\", (cashier_area[0], cashier_area[1] - 10),\n",
        "                FONT, 0.7, CASHIER_BOX_COLOR, 1)\n",
        "\n",
        "    # draw queue area\n",
        "    cv2.rectangle(annotated_frame,\n",
        "                  (queue_area[0], queue_area[1]),\n",
        "                  (queue_area[2], queue_area[3]),\n",
        "                  QUEUE_AREA_COLOR, QUEUE_AREA_THICKNESS)\n",
        "    cv2.putText(annotated_frame, \"Queue Area\", (queue_area[0], queue_area[1] - 10),\n",
        "                FONT, 0.7, QUEUE_AREA_COLOR, 1)\n",
        "\n",
        "    # draw divider line between zones\n",
        "    cv2.line(annotated_frame, (zone_divider, 0), (zone_divider, height), QUEUE_AREA_COLOR, 1)\n",
        "\n",
        "\n",
        "    # display counts on frame\n",
        "    cv2.putText(annotated_frame, f\"Camera 1 Person Count: {zone1_count}\", (20, 40),\n",
        "                FONT, COUNT_FONT_SCALE, ZONE1_COLOR, COUNT_FONT_THICKNESS)\n",
        "    cv2.putText(annotated_frame, f\"Camera 2 Person Count: {zone2_count}\", (20, 80),\n",
        "                FONT, COUNT_FONT_SCALE, ZONE2_COLOR, COUNT_FONT_THICKNESS)\n",
        "    cv2.putText(annotated_frame, f\"Total persons in Queue: {total_in_queue}\", (20, 120),\n",
        "                FONT, COUNT_FONT_SCALE, (255, 255, 255), COUNT_FONT_THICKNESS)\n",
        "\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(f\"✅ Updated annotated video with waiting time saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-18T20:11:18.980828Z",
          "iopub.status.busy": "2025-05-18T20:11:18.980538Z",
          "iopub.status.idle": "2025-05-18T20:12:00.528703Z",
          "shell.execute_reply": "2025-05-18T20:12:00.527704Z",
          "shell.execute_reply.started": "2025-05-18T20:11:18.980808Z"
        },
        "trusted": true,
        "id": "7TkIEkDqrzCq"
      },
      "outputs": [],
      "source": [
        "output_path = r\"/kaggle/working/step4_fully_customizable_output.mp4\"\n",
        "def display_middle_300(video_path):\n",
        "    reader = imageio.get_reader(video_path)\n",
        "    total_frames = reader.count_frames()\n",
        "    start_frame = max(0, total_frames // 2 - 150)\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    plt.axis('off')\n",
        "\n",
        "    mov = []\n",
        "    for i, frame in enumerate(reader):\n",
        "        if i < start_frame:\n",
        "            continue\n",
        "        if i >= start_frame + 300:\n",
        "            break\n",
        "        img = plt.imshow(frame, animated=True)\n",
        "        plt.axis('off')\n",
        "        mov.append([img])\n",
        "\n",
        "    reader.close()\n",
        "\n",
        "    anime = animation.ArtistAnimation(fig, mov, interval=50, repeat_delay=1000)\n",
        "    plt.close()\n",
        "    return anime\n",
        "\n",
        "HTML(display_middle_300(output_path).to_html5_video())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-18T20:14:45.975248Z",
          "iopub.status.busy": "2025-05-18T20:14:45.974970Z",
          "iopub.status.idle": "2025-05-18T20:15:27.598648Z",
          "shell.execute_reply": "2025-05-18T20:15:27.597848Z",
          "shell.execute_reply.started": "2025-05-18T20:14:45.975226Z"
        },
        "trusted": true,
        "id": "maDElHZJrzCr"
      },
      "outputs": [],
      "source": [
        "output_path = r\"/kaggle/input/clear-detection/clear_boundary.mp4\"\n",
        "def display_middle_300(video_path):\n",
        "    reader = imageio.get_reader(video_path)\n",
        "    total_frames = reader.count_frames()\n",
        "    start_frame = max(0, total_frames // 2 - 150)\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 6), dpi=100)\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    plt.axis('off')\n",
        "    mov = []\n",
        "    for i, frame in enumerate(reader):\n",
        "        if i < start_frame:\n",
        "            continue\n",
        "        if i >= start_frame + 300:\n",
        "            break\n",
        "        img = plt.imshow(frame, animated=True)\n",
        "        plt.axis('off')\n",
        "        mov.append([img])\n",
        "\n",
        "    reader.close()\n",
        "\n",
        "    anime = animation.ArtistAnimation(fig, mov, interval=50, repeat_delay=1000)\n",
        "    plt.close()\n",
        "    return anime\n",
        "\n",
        "HTML(display_middle_300(output_path).to_html5_video())"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7447709,
          "sourceId": 11862908,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 7452983,
          "sourceId": 11862998,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}